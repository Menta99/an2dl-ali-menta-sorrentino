{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MERGE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSd6GJm4Yhc"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n",
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras as keras\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "from PIL import Image\r\n",
        "import time\r\n",
        "import math\r\n",
        "from datetime import datetime\r\n",
        "from matplotlib import cm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "import cv2\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "TRAIN = True #True for training, False to load the model\r\n",
        "SEED = 1234\r\n",
        "tf.random.set_seed(SEED)  \r\n",
        "cwd = os.getcwd()\r\n",
        "num_classes = 3\r\n",
        "bs = 1\r\n",
        "img_h = 512\r\n",
        "img_w = 512\r\n",
        "up_h = 1024\r\n",
        "up_w = 1024 \r\n",
        "orig_img_h = 1536\r\n",
        "orig_img_w = 2048"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmxYLK2z5ChQ",
        "outputId": "957432c6-73c6-4b9f-ab5c-7135827a98d2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWup9jak4my6"
      },
      "source": [
        "def IoU(y_true, y_pred):\r\n",
        "  score_th = 0.5\r\n",
        "  y_pred = tf.cast(y_pred > score_th, tf.float32)\r\n",
        "    \r\n",
        "  intersection = tf.reduce_sum(y_true * y_pred)\r\n",
        "  union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\r\n",
        "    \r\n",
        "  return ((intersection + 1e-7) / (union + 1e-7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOy70TtW4ti_"
      },
      "source": [
        "# Load the Preliminary models\r\n",
        "modelWeed = tf.keras.models.load_model('/content/drive/My Drive/ANN/Models/Preliminary_UNET_weed', custom_objects={'IoU':IoU})\r\n",
        "modelCrop = tf.keras.models.load_model('/content/drive/My Drive/ANN/Models/Preliminary_UNET_crop', custom_objects={'IoU':IoU})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEajt8nb9gGW"
      },
      "source": [
        "# CustomDataset\r\n",
        "# -------------\r\n",
        "\r\n",
        "# Custom generator the yelds the RGB image and the converted mask (only with weed or crop)\r\n",
        "# in order to have the masks of the training/validation dataset predicted by the two \r\n",
        "# Preliminary networks\r\n",
        "class CustomDataset(tf.keras.utils.Sequence):\r\n",
        "  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \r\n",
        "               preprocessing_function=None, out_shape=[img_h, img_w], cult_type=None):\r\n",
        "    if which_subset == 'training':\r\n",
        "      subset_file = os.path.join(dataset_dir, 'Splits', 'train.txt')\r\n",
        "    elif which_subset == 'validation':\r\n",
        "      subset_file = os.path.join(dataset_dir, 'Splits', 'val.txt')\r\n",
        "    \r\n",
        "    with open(subset_file, 'r') as f:\r\n",
        "      lines = f.readlines()\r\n",
        "    \r\n",
        "    subset_filenames = []\r\n",
        "    for line in lines:\r\n",
        "      subset_filenames.append(line.strip()) \r\n",
        "\r\n",
        "    self.which_subset = which_subset\r\n",
        "    self.dataset_dir = dataset_dir\r\n",
        "    self.subset_filenames = subset_filenames\r\n",
        "    self.img_generator = img_generator\r\n",
        "    self.mask_generator = mask_generator\r\n",
        "    self.preprocessing_function = preprocessing_function\r\n",
        "    self.out_shape = out_shape\r\n",
        "    self.cult_type = cult_type\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.subset_filenames)\r\n",
        "\r\n",
        "  def _read_rgb_mask(self, img_path):\r\n",
        "    mask_img = Image.open(img_path)\r\n",
        "    mask_img = mask_img.resize(self.out_shape, resample=Image.NEAREST)\r\n",
        "    mask_arr = np.array(mask_img)\r\n",
        "\r\n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\r\n",
        "\r\n",
        "    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\r\n",
        "    if self.cult_type == 'weed':\r\n",
        "      new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 1\r\n",
        "    \r\n",
        "    elif self.cult_type == 'crop':\r\n",
        "      new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\r\n",
        "\r\n",
        "    return new_mask_arr\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    curr_filename = self.subset_filenames[index]\r\n",
        "    img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.jpg'))\r\n",
        "    img = img.resize(self.out_shape)\r\n",
        "\r\n",
        "    img_arr = np.array(img)\r\n",
        "\r\n",
        "    mask_arr = self._read_rgb_mask(os.path.join(self.dataset_dir, 'Masks', curr_filename + '.png'))\r\n",
        "\r\n",
        "    mask_arr = np.expand_dims(mask_arr, -1)\r\n",
        "    \r\n",
        "    if self.preprocessing_function is not None:\r\n",
        "        img_arr = self.preprocessing_function(img_arr)\r\n",
        "\r\n",
        "    return img_arr, np.float32(mask_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu3c4lkx9nyi"
      },
      "source": [
        "# Creation of the training and validation generators for the Preliminary Networks\r\n",
        "dataset_dir = os.path.join(cwd, 'drive/MyDrive/Assignment_2/Dataset')\r\n",
        "dataset_dir = os.path.join(dataset_dir, 'BipBip')\r\n",
        "\r\n",
        "dataset_crop = CustomDataset(dataset_dir, 'training', cult_type='crop',img_generator=None, mask_generator=None)\r\n",
        "\r\n",
        "dataset_valid_crop = CustomDataset(dataset_dir, 'validation', cult_type='crop', img_generator=None, mask_generator=None)\r\n",
        "\r\n",
        "train_dataset_crop = tf.data.Dataset.from_generator(lambda: dataset_crop,\r\n",
        "                                               output_types=(tf.float32, tf.float32),\r\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n",
        "train_dataset_crop = train_dataset_crop.batch(bs)\r\n",
        "\r\n",
        "valid_dataset_crop = tf.data.Dataset.from_generator(lambda: dataset_valid_crop,\r\n",
        "                                               output_types=(tf.float32, tf.float32),\r\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n",
        "valid_dataset_crop = valid_dataset_crop.batch(bs)\r\n",
        "\r\n",
        "dataset_weed = CustomDataset(dataset_dir, 'training', cult_type='weed',img_generator=None, mask_generator=None)\r\n",
        "\r\n",
        "dataset_valid_weed = CustomDataset(dataset_dir, 'validation', cult_type='weed',img_generator=None, mask_generator=None)\r\n",
        "\r\n",
        "train_dataset_weed = tf.data.Dataset.from_generator(lambda: dataset_weed,\r\n",
        "                                               output_types=(tf.float32, tf.float32),\r\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n",
        "train_dataset_weed = train_dataset_weed.batch(bs)\r\n",
        "\r\n",
        "valid_dataset_weed = tf.data.Dataset.from_generator(lambda: dataset_valid_weed,\r\n",
        "                                               output_types=(tf.float32, tf.float32),\r\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n",
        "valid_dataset_weed = valid_dataset_weed.batch(bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoOk-lOf9vQ_",
        "outputId": "21a40e67-e034-4e92-efd7-fe0f22e35b8d"
      },
      "source": [
        "# Creation of the masks to use in the training phase, respectively:\r\n",
        "# Train_crop, Train_weed, Valid_crop, Valid_weed\r\n",
        "train_crop_masks = modelCrop.predict(train_dataset_crop)\r\n",
        "result_train_crop = np.array(train_crop_masks)\r\n",
        "print('crop_train', result_train_crop.shape)\r\n",
        "valid_crop_masks = modelCrop.predict(valid_dataset_crop)\r\n",
        "result_valid_crop = np.array(valid_crop_masks)\r\n",
        "print('crop_valid', result_valid_crop.shape)\r\n",
        "train_weed_masks = modelWeed.predict(train_dataset_weed)\r\n",
        "result_train_weed = np.array(train_weed_masks)\r\n",
        "print('weed_train', result_train_weed.shape)\r\n",
        "valid_weed_masks = modelWeed.predict(valid_dataset_weed)\r\n",
        "result_valid_weed = np.array(valid_weed_masks)\r\n",
        "print('weed_valid', result_valid_weed.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crop_train (72, 512, 512, 1)\n",
            "crop_valid (18, 512, 512, 1)\n",
            "weed_train (72, 512, 512, 1)\n",
            "weed_valid (18, 512, 512, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HZGSYw_5dl_"
      },
      "source": [
        "# CustomDatasetMasks\r\n",
        "# -------------\r\n",
        "\r\n",
        "# Custom generator the yelds a five layer input (first 3 are the RGB image, the last 2 are the mask \r\n",
        "# generated by the preliminary networks) and the output mask\r\n",
        "class CustomDatasetMask(tf.keras.utils.Sequence):\r\n",
        "  def __init__(self, dataset_dir, which_subset, out_shape=[up_h, up_w], cult_type=None, weed=None, crop=None):\r\n",
        "    if which_subset == 'training':\r\n",
        "      subset_file = os.path.join(dataset_dir, 'Splits', 'train.txt')\r\n",
        "    elif which_subset == 'validation':\r\n",
        "      subset_file = os.path.join(dataset_dir, 'Splits', 'val.txt')\r\n",
        "    \r\n",
        "    with open(subset_file, 'r') as f:\r\n",
        "      lines = f.readlines()\r\n",
        "    \r\n",
        "    subset_filenames = []\r\n",
        "    for line in lines:\r\n",
        "      subset_filenames.append(line.strip()) \r\n",
        "\r\n",
        "    self.which_subset = which_subset\r\n",
        "    self.dataset_dir = dataset_dir\r\n",
        "    self.subset_filenames = subset_filenames\r\n",
        "    self.out_shape = out_shape\r\n",
        "    self.cult_type = cult_type\r\n",
        "    self.weed = weed\r\n",
        "    self.crop = crop \r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.subset_filenames)\r\n",
        "\r\n",
        "  def _read_rgb_mask(self, img_path):\r\n",
        "    mask_img = Image.open(img_path)\r\n",
        "    mask_img = mask_img.resize(self.out_shape, resample=Image.NEAREST)\r\n",
        "    mask_arr = np.array(mask_img)\r\n",
        "\r\n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\r\n",
        "\r\n",
        "    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\r\n",
        "    if self.cult_type == 'weed':\r\n",
        "      new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 1\r\n",
        "    \r\n",
        "    elif self.cult_type == 'crop':\r\n",
        "      new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\r\n",
        "    \r\n",
        "    elif self.cult_type == 'merge':\r\n",
        "      new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\r\n",
        "      new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\r\n",
        "\r\n",
        "    return new_mask_arr\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    curr_filename = self.subset_filenames[index]\r\n",
        "    mask_arr = self._read_rgb_mask(os.path.join(self.dataset_dir, 'Masks', curr_filename + '.png'))\r\n",
        "    mask_arr = np.expand_dims(mask_arr, -1) \r\n",
        "\r\n",
        "    img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.jpg'))\r\n",
        "    img = img.resize(self.out_shape)\r\n",
        "    img_arr = np.array(img)\r\n",
        "\r\n",
        "    return (img_arr, self.weed[index], self.crop[index]), np.float32(mask_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuoXE_h25Whp"
      },
      "source": [
        "# ImageDataGenerator\r\n",
        "# ------------------\r\n",
        "apply_data_augmentation = True\r\n",
        "\r\n",
        "if apply_data_augmentation:\r\n",
        "    img_data_gen = ImageDataGenerator(rotation_range=30,\r\n",
        "                                      width_shift_range=30,\r\n",
        "                                      height_shift_range=30,\r\n",
        "                                      zoom_range=0.4,\r\n",
        "                                      horizontal_flip=True,\r\n",
        "                                      vertical_flip=True,\r\n",
        "                                      fill_mode='reflect')\r\n",
        "    mask_data_gen = ImageDataGenerator(rotation_range=30,\r\n",
        "                                       width_shift_range=30,\r\n",
        "                                       height_shift_range=30,\r\n",
        "                                       zoom_range=0.4,\r\n",
        "                                       horizontal_flip=True,\r\n",
        "                                       vertical_flip=True,\r\n",
        "                                       fill_mode='reflect')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWrhcxonW74U"
      },
      "source": [
        "# Creation of the training and validation generators for the Merge Network\r\n",
        "dataset_dir = os.path.join(cwd, 'drive/MyDrive/Assignment_2/Dataset')\r\n",
        "dataset_dir = os.path.join(dataset_dir, 'BipBip')\r\n",
        "\r\n",
        "dataset_train_img = CustomDatasetMask(dataset_dir, 'training', cult_type='merge', weed=result_train_weed, crop=result_train_crop)\r\n",
        "dataset_valid_img = CustomDatasetMask(dataset_dir, 'validation', cult_type='merge', weed=result_valid_weed, crop=result_valid_crop)\r\n",
        "\r\n",
        "types = ((tf.float32, tf.float32, tf.float32), tf.float32)\r\n",
        "shapes = (([up_h, up_w, 3], [img_h, img_w, 1], [img_h, img_w, 1]), [up_h, up_w, 1])\r\n",
        "\r\n",
        "train_img = tf.data.Dataset.from_generator(lambda: dataset_train_img, output_types=types ,output_shapes=shapes)\r\n",
        "train_img = train_img.batch(bs)\r\n",
        "train_img = train_img.repeat()\r\n",
        "valid_img = tf.data.Dataset.from_generator(lambda: dataset_valid_img, output_types=types ,output_shapes=shapes)\r\n",
        "valid_img = valid_img.batch(bs)\r\n",
        "valid_img = valid_img.repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtD1KXj-Ssts"
      },
      "source": [
        "# Creation of the model of the network:\r\n",
        "# it's a custom version of the UNet, with Conv2D (with He-uniform initialization), \r\n",
        "# BatchNormalization and LeakyReLu layers\r\n",
        "def MERGE(input_size = (up_h,up_w,5)):\r\n",
        "    image_input = keras.Input(shape=(up_h, up_w, 3), name=\"img_input\")\r\n",
        "    weed_mask = keras.Input(shape=(img_h, img_w, 1), name=\"weed_input\")\r\n",
        "    weed_upscale = keras.layers.UpSampling2D(size = (2,2))(weed_mask)\r\n",
        "    crop_mask = keras.Input(shape=(img_h, img_w, 1), name=\"crop_input\")\r\n",
        "    crop_upscale = keras.layers.UpSampling2D(size = (2,2))(crop_mask)\r\n",
        "    inputs = keras.layers.concatenate([image_input,weed_upscale,crop_upscale], axis=-1)\r\n",
        "    \r\n",
        "    conv1 = keras.layers.Conv2D(16, 3, padding='same', kernel_initializer = 'he_uniform')(inputs)\r\n",
        "    bano1 = keras.layers.BatchNormalization()(conv1)\r\n",
        "    acti1 = keras.layers.LeakyReLU()(bano1)\r\n",
        "    pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(acti1)\r\n",
        "\r\n",
        "    conv2 = keras.layers.Conv2D(32, 3, padding='same', kernel_initializer = 'he_uniform')(pool1)\r\n",
        "    bano2 = keras.layers.BatchNormalization()(conv2)\r\n",
        "    acti2 = keras.layers.LeakyReLU()(bano2)\r\n",
        "    pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(acti2)\r\n",
        "\r\n",
        "    conv3 = keras.layers.Conv2D(64, 3, padding='same', kernel_initializer = 'he_uniform')(pool2)\r\n",
        "    bano3 = keras.layers.BatchNormalization()(conv3)\r\n",
        "    acti3 = keras.layers.LeakyReLU()(bano3)\r\n",
        "    pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(acti3)\r\n",
        "\r\n",
        "    conv4 = keras.layers.Conv2D(128, 3, padding='same', kernel_initializer = 'he_uniform')(pool3)\r\n",
        "    bano4 = keras.layers.BatchNormalization()(conv4)\r\n",
        "    acti4 = keras.layers.LeakyReLU()(bano4)\r\n",
        "    pool4 = keras.layers.MaxPooling2D(pool_size=(2, 2))(acti4)\r\n",
        "\r\n",
        "    conv5 = keras.layers.Conv2D(256, 3, padding='same', kernel_initializer = 'he_uniform')(pool4)\r\n",
        "    bano5 = keras.layers.BatchNormalization()(conv5)\r\n",
        "    acti5 = keras.layers.LeakyReLU()(bano5)\r\n",
        "    pool5 = keras.layers.MaxPooling2D(pool_size=(2, 2))(acti5)\r\n",
        "\r\n",
        "    ###################################################################################################################\r\n",
        "    conv6 = keras.layers.Conv2D(512, 3, padding = 'same', kernel_initializer = 'he_uniform')(pool5)\r\n",
        "    bano6 = keras.layers.BatchNormalization()(conv6)\r\n",
        "    acti6 = keras.layers.LeakyReLU()(bano6)\r\n",
        "    ###################################################################################################################\r\n",
        "\r\n",
        "    u0=keras.layers.UpSampling2D(size = (2,2))(acti6)\r\n",
        "    c0 = keras.layers.Conv2D(256, 2, padding='same', kernel_initializer = 'he_uniform')(u0)\r\n",
        "    c0 = keras.layers.concatenate([acti5,c0], axis = 3)\r\n",
        "    c0 = keras.layers.Conv2D(256, 3, padding='same', kernel_initializer = 'he_uniform')(c0)\r\n",
        "    b0 = keras.layers.BatchNormalization()(c0)\r\n",
        "    a0 = keras.layers.LeakyReLU()(b0)\r\n",
        "\r\n",
        "    u1=keras.layers.UpSampling2D(size = (2,2))(a0)\r\n",
        "    c1 = keras.layers.Conv2D(128, 2, padding='same', kernel_initializer = 'he_uniform')(u1)\r\n",
        "    c1 = keras.layers.concatenate([acti4,c1], axis = 3)\r\n",
        "    c1 = keras.layers.Conv2D(128, 3, padding='same', kernel_initializer = 'he_uniform')(c1)\r\n",
        "    b1 = keras.layers.BatchNormalization()(c1)\r\n",
        "    a1 = keras.layers.LeakyReLU()(b1)\r\n",
        "\r\n",
        "    u2=keras.layers.UpSampling2D(size = (2,2))(a1)\r\n",
        "    c2 = keras.layers.Conv2D(64, 2, padding='same', kernel_initializer = 'he_uniform')(u2)\r\n",
        "    c2 =keras.layers.concatenate([acti3,c2], axis = 3)\r\n",
        "    c2 = keras.layers.Conv2D(64, 3, padding='same', kernel_initializer = 'he_uniform')(c2)\r\n",
        "    b2 = keras.layers.BatchNormalization()(c2)\r\n",
        "    a2 = keras.layers.LeakyReLU()(b2)\r\n",
        "\r\n",
        "    u3=keras.layers.UpSampling2D(size = (2,2))(a2)\r\n",
        "    c3 = keras.layers.Conv2D(32, 2, padding='same', kernel_initializer = 'he_uniform')(u3)\r\n",
        "    c3 = keras.layers.concatenate([acti2,c3], axis = 3)\r\n",
        "    c3 = keras.layers.Conv2D(32, 3, padding='same', kernel_initializer = 'he_uniform')(c3)\r\n",
        "    b3 = keras.layers.BatchNormalization()(c3)\r\n",
        "    a3 = keras.layers.LeakyReLU()(b3)\r\n",
        "\r\n",
        "    u4=keras.layers.UpSampling2D(size = (2,2))(a3)\r\n",
        "    c4 = keras.layers.Conv2D(16, 2, padding='same', kernel_initializer = 'he_uniform')(u4)\r\n",
        "    c4 =keras.layers.concatenate([acti1,c4], axis = 3)\r\n",
        "    c4 = keras.layers.Conv2D(16, 3, padding='same', kernel_initializer = 'he_uniform')(c4)\r\n",
        "    b4 = keras.layers.BatchNormalization()(c4)\r\n",
        "    a4 = keras.layers.LeakyReLU()(b4)\r\n",
        "    out =keras.layers.Conv2D(num_classes, 1, activation = 'softmax', name=\"output\")(a4)\r\n",
        "\r\n",
        "    model = keras.models.Model(inputs=[image_input,weed_mask,crop_mask], outputs=out)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vzc9h1JUF8A"
      },
      "source": [
        "model = MERGE()\r\n",
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yp6PjeDUQpL"
      },
      "source": [
        "# Loss\r\n",
        "# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "lr = 1e-3\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n",
        "# -------------------\r\n",
        "\r\n",
        "# Here we define the intersection over union for each class in the batch.\r\n",
        "# Then we compute the final iou as the mean over classes\r\n",
        "def meanIoU(y_true, y_pred):\r\n",
        "    # get predicted class from softmax\r\n",
        "    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\r\n",
        "    weights = [1, 1]\r\n",
        "    weights_sum = sum(weights)\r\n",
        "    per_class_iou = []\r\n",
        "\r\n",
        "    for i in range(1,num_classes): # exclude the background class 0\r\n",
        "      # Get prediction and target related to only a single class (i)\r\n",
        "      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\r\n",
        "      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\r\n",
        "      intersection = tf.reduce_sum(class_true * class_pred)\r\n",
        "      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\r\n",
        "    \r\n",
        "      iou = (weights[i-1] * (intersection + 1e-7)) / ((union + 1e-7) * weights_sum)\r\n",
        "      per_class_iou.append(iou)\r\n",
        "\r\n",
        "    return tf.reduce_sum(per_class_iou)\r\n",
        "# Validation metrics\r\n",
        "# ------------------\r\n",
        "metrics = ['accuracy', meanIoU]\r\n",
        "# ------------------\r\n",
        "\r\n",
        "# Compile Model\r\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00QAuTXLUsma"
      },
      "source": [
        "%reload_ext tensorboard\r\n",
        "%tensorboard --logdir /content/drive/My\\ Drive/ANN/Log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYC4BfT3VVSM"
      },
      "source": [
        "if(TRAIN):\r\n",
        "  exps_dir = os.path.join(cwd, 'drive/My Drive/ANN/Log/Waterfall')\r\n",
        "  if not os.path.exists(exps_dir):\r\n",
        "      os.makedirs(exps_dir)\r\n",
        "\r\n",
        "  now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n",
        "\r\n",
        "  model_name = 'MERGE_CoBaLeRu'\r\n",
        "\r\n",
        "  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\r\n",
        "  if not os.path.exists(exp_dir):\r\n",
        "      os.makedirs(exp_dir)\r\n",
        "      \r\n",
        "  callbacks = []\r\n",
        "\r\n",
        "  # Model checkpoint\r\n",
        "  # ----------------\r\n",
        "  ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n",
        "  if not os.path.exists(ckpt_dir):\r\n",
        "      os.makedirs(ckpt_dir)\r\n",
        "\r\n",
        "  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \r\n",
        "                                                    save_weights_only=True, save_best_only=True)  # False to save the model directly\r\n",
        "  callbacks.append(ckpt_callback)\r\n",
        "\r\n",
        "  # Visualize Learning on Tensorboard\r\n",
        "  # ---------------------------------\r\n",
        "  tb_dir = os.path.join(exp_dir, 'tb_logs')\r\n",
        "  if not os.path.exists(tb_dir):\r\n",
        "      os.makedirs(tb_dir)\r\n",
        "      \r\n",
        "  # By default shows losses and metrics for both training and validation\r\n",
        "  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\r\n",
        "                                              profile_batch=0,\r\n",
        "                                              histogram_freq=0)  # if 1 shows weights histograms\r\n",
        "  callbacks.append(tb_callback)\r\n",
        "\r\n",
        "  reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1, cooldown=0)\r\n",
        "  callbacks.append(reduce_lr)\r\n",
        "\r\n",
        "  # Early Stopping\r\n",
        "  # --------------\r\n",
        "  early_stop = True\r\n",
        "  if early_stop:\r\n",
        "      es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\r\n",
        "      callbacks.append(es_callback)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  model.fit(train_img,\r\n",
        "            epochs=100,  #### set repeat in training dataset\r\n",
        "            steps_per_epoch=len(dataset_train_img),\r\n",
        "            validation_data=valid_img,\r\n",
        "            validation_steps=len(dataset_valid_img),\r\n",
        "            callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snZbMI8dmPUx"
      },
      "source": [
        "if(TRAIN):\r\n",
        "  # Save the model\r\n",
        "  model.save('/content/drive/My Drive/ANN/Models/MERGE')\r\n",
        "else:\r\n",
        "  model = tf.keras.models.load_model('/content/drive/My Drive/ANN/Models/MERGE', custom_objects={'meanIoU':meanIoU})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC3Uah7F0Qrf"
      },
      "source": [
        "# CustomTest\r\n",
        "# -------------\r\n",
        "\r\n",
        "# Custom generator the yelds a five layer input (first 3 are the RGB image, the last 2 are the mask \r\n",
        "# generated by the preliminary networks) for the testing phase\r\n",
        "class CustomTest(tf.keras.utils.Sequence):\r\n",
        "  def __init__(self, dataset_dir, filenames, out_shape=[up_h, up_w], weed=None, crop=None):\r\n",
        "    self.dataset_dir = dataset_dir\r\n",
        "    self.filenames = filenames\r\n",
        "    self.out_shape = out_shape\r\n",
        "    self.weed = weed\r\n",
        "    self.crop = crop \r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.filenames)\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    curr_filename = self.filenames[index] \r\n",
        "    mypath = os.path.join(self.dataset_dir, 'Images', curr_filename + '.jpg') \r\n",
        "    if os.path.exists(mypath):\r\n",
        "      img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.jpg'))\r\n",
        "    else :\r\n",
        "      img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.png'))\r\n",
        "    img = img.resize(self.out_shape)\r\n",
        "    img_arr = np.array(img)\r\n",
        "\r\n",
        "    return (img_arr, self.weed[index], self.crop[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMYMJz2Ak1Eb"
      },
      "source": [
        "# Utility functions to produce the output file for the challenge\r\n",
        "def rle_encode(img):\r\n",
        "    pixels = img.flatten()\r\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\r\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\r\n",
        "    runs[1::2] -= runs[::2]\r\n",
        "    return ' '.join(str(x) for x in runs)\r\n",
        "\r\n",
        "################################################################################\r\n",
        "################################################################################\r\n",
        "\r\n",
        "def update_dictionary(submission_dict, img, img_name, team, crop):\r\n",
        "  submission_dict[img_name] = {}\r\n",
        "  submission_dict[img_name]['shape'] = img.shape\r\n",
        "  submission_dict[img_name]['team'] = team\r\n",
        "  submission_dict[img_name]['crop'] = crop\r\n",
        "  submission_dict[img_name]['segmentation'] = {}\r\n",
        "\r\n",
        "  # RLE encoding  \r\n",
        "  # crop\r\n",
        "  rle_encoded_crop = rle_encode(img == 1)\r\n",
        "  # weed\r\n",
        "  rle_encoded_weed = rle_encode(img == 2)\r\n",
        "\r\n",
        "  submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\r\n",
        "  submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\r\n",
        "  # Finally, save the results into the submission.json file\r\n",
        "  return submission_dict\r\n",
        "\r\n",
        "################################################################################\r\n",
        "################################################################################\r\n",
        "\r\n",
        "def trim_filenames(test_gen):\r\n",
        "  filenames = test_gen.filenames\r\n",
        "  prova = [e[7:] for e in filenames]\r\n",
        "  filenames = []\r\n",
        "  for file in prova:\r\n",
        "    filenames.append(file.split(\".\")[0])\r\n",
        "  return filenames\r\n",
        "\r\n",
        "################################################################################\r\n",
        "################################################################################\r\n",
        "\r\n",
        "def create_prediction(category, dictionary):\r\n",
        "  team = category.split('/')[0]\r\n",
        "  crop = category.split('/')[1]\r\n",
        "  path = os.path.join(cwd, '/content/drive/My Drive/Assignment_2/Dataset/All/Test_Dev', category)\r\n",
        "  print(path)\r\n",
        "  test_data_gen1 = ImageDataGenerator()\r\n",
        "  test_gen1 = test_data_gen1.flow_from_directory(path,\r\n",
        "                                               target_size=(img_h,img_w),\r\n",
        "                                               color_mode=\"rgb\",\r\n",
        "                                               batch_size=1, \r\n",
        "                                               class_mode='categorical',\r\n",
        "                                               classes=None,\r\n",
        "                                               shuffle=False,\r\n",
        "                                               seed=SEED)\r\n",
        "  \r\n",
        "  test_data_gen2 = ImageDataGenerator()\r\n",
        "  test_gen2 = test_data_gen2.flow_from_directory(path,\r\n",
        "                                               target_size=(img_h,img_w),\r\n",
        "                                               color_mode=\"rgb\",\r\n",
        "                                               batch_size=1, \r\n",
        "                                               class_mode='categorical',\r\n",
        "                                               classes=None,\r\n",
        "                                               shuffle=False,\r\n",
        "                                               seed=SEED)\r\n",
        "  \r\n",
        "  filenames = trim_filenames(test_gen1)\r\n",
        "  weed_pred = modelWeed.predict(test_gen1)\r\n",
        "  weed_pred = np.array(weed_pred)\r\n",
        "  crop_pred = modelCrop.predict(test_gen2)\r\n",
        "  crop_pred = np.array(crop_pred)\r\n",
        "  test = CustomTest(path, filenames=filenames, weed=weed_pred, crop=crop_pred)\r\n",
        "  test_img = tf.data.Dataset.from_generator(lambda: test, \r\n",
        "                                            output_types=(tf.float32, tf.float32, tf.float32),\r\n",
        "                                            output_shapes=([up_h, up_w, 3], [img_h, img_w, 1], [img_h, img_w, 1]))\r\n",
        "  test_img = test_img.batch(bs)\r\n",
        "  iterator = iter(test_img)\r\n",
        "  for i in range(len(filenames)):\r\n",
        "    pred = model.predict(iterator.next())\r\n",
        "    arr_ = np.squeeze(pred)\r\n",
        "    img = cv2.resize(arr_, dsize=(orig_img_w, orig_img_h), interpolation=cv2.INTER_CUBIC)\r\n",
        "    predicted_class = tf.argmax(img, -1)\r\n",
        "    result = np.array(predicted_class)\r\n",
        "    dictionary = update_dictionary(submission_dict=dictionary, img=result, img_name=filenames[i], team=team, crop=crop)\r\n",
        "  return dictionary\r\n",
        "\r\n",
        "################################################################################\r\n",
        "################################################################################\r\n",
        "\r\n",
        "def iterate_dataset(directories_path):\r\n",
        "  with open(directories_path, 'r') as f:\r\n",
        "    paths = [line.rstrip() for line in f.readlines()]\r\n",
        "    dictionary = {}\r\n",
        "    for name in paths:\r\n",
        "      dictionary = create_prediction(name, dictionary)\r\n",
        "    with open(os.path.join(cwd, 'drive/MyDrive/ANN/submission.json'), 'w') as f:\r\n",
        "      json.dump(dictionary, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTVvHIqslkHg"
      },
      "source": [
        "# Creation of the JSON output file\r\n",
        "iterate_dataset(os.path.join(cwd, '/content/drive/My Drive/ANN/directories.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}