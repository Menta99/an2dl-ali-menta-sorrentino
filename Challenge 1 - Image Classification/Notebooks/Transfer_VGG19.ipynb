{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge 1 - Model B (transfer VGG19).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vi_2gUjfSnU"
      },
      "source": [
        "###**Model B (transfer, VGG19) HYPERPARAMETERS** _(~95.5% on test set)_\n",
        "\n",
        "> SEED: 1234\n",
        "\n",
        "> train_dataset: 80%\n",
        "\n",
        "> valid_dataset: 20%\n",
        "\n",
        "> Batch size: 16\n",
        "\n",
        "> Image w/h: 612\n",
        "\n",
        "> Data Augmentation : Custom\n",
        "\n",
        "> CNN structure:\n",
        ">> VGG19\n",
        ">\n",
        ">> Only Block5 trainable\n",
        "\n",
        "> FCN structure:\n",
        ">> Flatten layer\n",
        ">\n",
        ">> Dense 1024 + HE init\n",
        ">\n",
        ">> Batch Normalization (Standard)\n",
        ">\n",
        ">> Activation : LeakyReLu\n",
        ">\n",
        ">> Dropout (0.7, seed= SEED)\n",
        ">\n",
        ">> Dense 512 + HE init\n",
        ">\n",
        ">> Batch Normalization (Standard)\n",
        ">\n",
        ">> Activation : LeakyReLu\n",
        ">\n",
        ">> Dropout (0.5, seed= SEED)\n",
        ">\n",
        ">> Dense 256 + He init\n",
        ">\n",
        ">> Batch Normalization (Standard)\n",
        ">\n",
        ">> Activation : LeakyReLu\n",
        ">\n",
        ">> Dropout (0.3, seed= SEED)\n",
        ">\n",
        ">> 3 neurons (Softmax)\n",
        "\n",
        "> Training parameters:\n",
        ">> Optimizer: Adam\n",
        ">\n",
        ">> Epochs: 50\n",
        ">\n",
        ">> Early stopping: 10 epochs\n",
        ">\n",
        ">> Learning rate: 1e-3\n",
        ">\n",
        ">> Loss: Categorical crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYShipKlfKtP"
      },
      "source": [
        "# Helper libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "\n",
        "# Fixed a seed to make results reproducible \n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Getting current main directory\n",
        "cwd = os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0kXQ6Ufg5Bx"
      },
      "source": [
        "# Mounting G Drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ5zufCgd_w_"
      },
      "source": [
        "# CONSTANTS\n",
        "bs = 16\n",
        "img_h = 612\n",
        "img_w = 612\n",
        "num_classes = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG1xqxvJiggi"
      },
      "source": [
        "dataset_dir = os.path.join(cwd, '/content/drive/My Drive/ANN/Dataset')\n",
        "training_dir = os.path.join(dataset_dir, 'training')\n",
        "valid_dir = os.path.join(dataset_dir, 'validation')\n",
        "test_dir = os.path.join(dataset_dir, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTA4suUsgQvJ"
      },
      "source": [
        "# Data augmentation\n",
        "apply_data_augmentation = True\n",
        "\n",
        "# Creating training ImageDataGenerator object\n",
        "if apply_data_augmentation:\n",
        "    train_data_gen = ImageDataGenerator(rotation_range=15,\n",
        "                                        width_shift_range=30,\n",
        "                                        height_shift_range=30,\n",
        "                                        zoom_range=[1,1.4],\n",
        "                                        shear_range=20,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=False,\n",
        "                                        brightness_range=[0.2,1.3],\n",
        "                                        fill_mode='nearest',\n",
        "                                        #cval=0,\n",
        "                                        preprocessing_function=preprocess_input)\n",
        "else:\n",
        "    train_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "valid_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "test_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQf-B2kFij_a"
      },
      "source": [
        "# Taking the path to a directory and generating batches of augmented data\n",
        "train_gen = train_data_gen.flow_from_directory(training_dir,\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               color_mode=\"rgb\",\n",
        "                                               batch_size=bs, \n",
        "                                               class_mode='categorical',\n",
        "                                               classes=['NO_MASK','ALL_MASK','SOME_MASK'],\n",
        "                                               shuffle=True,\n",
        "                                               seed=SEED)\n",
        "\n",
        "valid_gen = valid_data_gen.flow_from_directory(valid_dir,\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               color_mode=\"rgb\",\n",
        "                                               batch_size=bs, \n",
        "                                               class_mode='categorical',\n",
        "                                               classes=['NO_MASK','ALL_MASK','SOME_MASK'],\n",
        "                                               shuffle=False,\n",
        "                                               seed=SEED)\n",
        "\n",
        "test_gen = test_data_gen.flow_from_directory(test_dir,\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               color_mode=\"rgb\",\n",
        "                                               batch_size=1, \n",
        "                                               class_mode='categorical',\n",
        "                                               classes=None,\n",
        "                                               shuffle=False,\n",
        "                                               seed=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTwZ3OdieYbw"
      },
      "source": [
        "# Transfer learning\n",
        "vgg = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
        "\n",
        "finetuning = True\n",
        "\n",
        "if finetuning:\n",
        "    freeze_until = 17 # layer from which we want to fine-tune\n",
        "    for layer in vgg.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "else:\n",
        "    vgg.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzQTJsQV8QGZ"
      },
      "source": [
        "# Model    \n",
        "model = tf.keras.Sequential()\n",
        "model.add(vgg)\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units=1024, kernel_initializer='he_uniform'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.LeakyReLU())\n",
        "model.add(tf.keras.layers.Dropout(0.7, seed=SEED))\n",
        "model.add(tf.keras.layers.Dense(units=512, kernel_initializer='he_uniform'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.LeakyReLU())\n",
        "model.add(tf.keras.layers.Dropout(0.5, seed=SEED))\n",
        "model.add(tf.keras.layers.Dense(units=256, kernel_initializer='he_uniform'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.LeakyReLU())\n",
        "model.add(tf.keras.layers.Dropout(0.3, seed=SEED))\n",
        "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK7R6v1PeeNs"
      },
      "source": [
        "# Optimization params\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "lr = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "metrics = ['accuracy']\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiGKCwOTjQHa"
      },
      "source": [
        "# Visualize created model as a table\n",
        "model.summary()\n",
        "for i, layer in enumerate(vgg.layers):\n",
        "  print(i, layer.name, \"-\", layer.trainable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WfGM1flj8Gn"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/My\\ Drive/ANN/Log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtXAkhDIkC3b"
      },
      "source": [
        "# Callbacks\n",
        "cwd = '/content/drive/My Drive/ANN'\n",
        "\n",
        "exps_dir = os.path.join(cwd, 'Log')\n",
        "\n",
        "if not os.path.exists(exps_dir):\n",
        "  os.makedirs(exps_dir)\n",
        "\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S') # taking instant time\n",
        "exp_name = 'FC'                                # name of experiment\n",
        "exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n",
        "\n",
        "\n",
        "if not os.path.exists(exp_dir):\n",
        "  os.makedirs(exp_dir)\n",
        "\n",
        "\n",
        "callbacks = [] \n",
        "\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), save_weights_only=True, save_best_only=True)\n",
        "\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
        "if not os.path.exists(tb_dir):\n",
        "  os.makedirs(tb_dir)\n",
        "\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=1)\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "\n",
        "# Early Stopping\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "  callbacks.append(es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFw01ykWkjys"
      },
      "source": [
        "### training the model ###\n",
        "model.fit(\n",
        "    x=train_gen,\n",
        "    y=None,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_data=valid_gen,\n",
        "    validation_steps=len(valid_gen)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd80MGYSlE0x"
      },
      "source": [
        "# OPTIONAL (model saving)\n",
        "model.save('/content/drive/My Drive/ANN/Models/Saved_VGG19_J')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvRzPOr9k7oB"
      },
      "source": [
        "# Helper function used for creating csv file\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'MENTA_VGG19_J_results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bayLst1HlMr3"
      },
      "source": [
        "# Predicting the model and writing result into a csv by calling create_csv function\n",
        "results = {}\n",
        "test_gen.reset()\n",
        "predictions = model.predict(test_gen,\n",
        "                            batch_size = bs,\n",
        "                            verbose=1)\n",
        "\n",
        "predicted_class_indices=np.argmax(predictions,axis=1)\n",
        "\n",
        "filenames = test_gen.filenames\n",
        "\n",
        "prova = [e[7:] for e in filenames]\n",
        "\n",
        "for i in range(0, len(test_gen)):\n",
        "  results[prova[i]] = predicted_class_indices[i]\n",
        "\n",
        "create_csv(results=results, results_dir=test_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}