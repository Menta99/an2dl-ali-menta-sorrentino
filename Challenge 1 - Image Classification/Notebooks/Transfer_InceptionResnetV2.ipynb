{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge 1 - Model C (transfer, inc_res_v2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ0I1ritEq1r"
      },
      "source": [
        "###**Model C (transfer, inc_res_v2) HYPERPARAMETERS** _(~94% on test set)_**\n",
        "\n",
        "> SEED: 1234\n",
        "\n",
        "> train_dataset: 80%\n",
        "\n",
        "> valid_dataset: 20%\n",
        "\n",
        "> Batch size: 32\n",
        "\n",
        "> Image w/h: 299\n",
        "\n",
        "> Data Augmentation : Custom\n",
        "\n",
        "> CNN structure:\n",
        ">> Inception Resnet V2\n",
        ">\n",
        ">> All Trainable\n",
        "\n",
        "> FCN structure:\n",
        ">> Global Average Pooling layer\n",
        ">\n",
        ">> Dense 1024 + HE init\n",
        ">\n",
        ">> Batch Normalization (Standard)\n",
        ">\n",
        ">> Activation : LeakyReLu\n",
        ">\n",
        ">> Dropout (0.5, seed= SEED)\n",
        ">\n",
        ">> Dense 256 + HE init\n",
        ">\n",
        ">> Batch Normalization (Standard)\n",
        ">\n",
        ">> Activation : LeakyReLu\n",
        ">\n",
        ">> Dropout (0.5, seed= SEED)\n",
        ">\n",
        ">> 3 neurons (Softmax)\n",
        "\n",
        "> Training parameters:\n",
        ">> Optimizer: Adam\n",
        ">\n",
        ">> Epochs: 50\n",
        ">\n",
        ">> Early stopping: 10 epochs\n",
        ">\n",
        ">> Learning rate: 1e-3\n",
        ">\n",
        ">> Loss: Categorical crossentropy\n",
        ">\n",
        ">> ReduceLROnPlateau\n",
        ">\n",
        ">> Test Time Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYShipKlfKtP"
      },
      "source": [
        "# Helper libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.inception_resnet_v2 import preprocess_input\n",
        "\n",
        "# Fixed a seed to make results reproducible \n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Getting current main directory\n",
        "cwd = os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0kXQ6Ufg5Bx"
      },
      "source": [
        "# Mounting G Drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T37VJXWcFgX4"
      },
      "source": [
        "# CONSTANTS\n",
        "bs = 32\n",
        "img_h = 299\n",
        "img_w = 299\n",
        "num_classes = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG1xqxvJiggi"
      },
      "source": [
        "dataset_dir = os.path.join(cwd, '/content/drive/My Drive/ANN/Dataset')\n",
        "training_dir = os.path.join(dataset_dir, 'training')\n",
        "valid_dir = os.path.join(dataset_dir, 'validation')\n",
        "test_dir = os.path.join(dataset_dir, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTA4suUsgQvJ"
      },
      "source": [
        "# Data augmentation\n",
        "apply_data_augmentation = True\n",
        "\n",
        "# Creating training ImageDataGenerator object\n",
        "if apply_data_augmentation:\n",
        "    train_data_gen = ImageDataGenerator(rotation_range=20,\n",
        "                                        width_shift_range=20,\n",
        "                                        height_shift_range=20,\n",
        "                                        channel_shift_range=20,\n",
        "                                        zoom_range=[0.8,1.2],\n",
        "                                        shear_range=10,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=False,\n",
        "                                        brightness_range=[0.6,1.4],\n",
        "                                        fill_mode='nearest',\n",
        "                                        preprocessing_function=preprocess_input )\n",
        "else:\n",
        "    train_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "valid_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "test_data_gen = ImageDataGenerator(rotation_range=20,\n",
        "                                        width_shift_range=20,\n",
        "                                        height_shift_range=20,\n",
        "                                        channel_shift_range=20,\n",
        "                                        zoom_range=[0.8,1.2],\n",
        "                                        shear_range=10,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=False,\n",
        "                                        brightness_range=[0.6,1.4],\n",
        "                                        fill_mode='nearest',\n",
        "                                        preprocessing_function=preprocess_input )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQf-B2kFij_a"
      },
      "source": [
        "# Taking the path to a directory and generating batches of augmented data\n",
        "train_gen = train_data_gen.flow_from_directory(training_dir,\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               color_mode=\"rgb\",\n",
        "                                               batch_size=bs, \n",
        "                                               class_mode='categorical',\n",
        "                                               classes=['NO_MASK','ALL_MASK','SOME_MASK'],\n",
        "                                               shuffle=True,\n",
        "                                               seed=SEED)\n",
        "\n",
        "valid_gen = valid_data_gen.flow_from_directory(valid_dir,\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               color_mode=\"rgb\",\n",
        "                                               batch_size=bs, \n",
        "                                               class_mode='categorical',\n",
        "                                               classes=['NO_MASK','ALL_MASK','SOME_MASK'],\n",
        "                                               shuffle=False,\n",
        "                                               seed=SEED)\n",
        "\n",
        "test_gen = test_data_gen.flow_from_directory(test_dir,\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               color_mode=\"rgb\",\n",
        "                                               batch_size=1, \n",
        "                                               class_mode='categorical',\n",
        "                                               classes=None,\n",
        "                                               shuffle=False,\n",
        "                                               seed=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uelKUSgI8xvs"
      },
      "source": [
        "# Transfer learning\n",
        "inc_res = tf.keras.applications.InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
        "\n",
        "finetuning = True\n",
        "\n",
        "if finetuning:\n",
        "    inc_res.trainable = True\n",
        "else:\n",
        "    inc_res.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzQTJsQV8QGZ"
      },
      "source": [
        "#Model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(inc_res)\n",
        "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "model.add(tf.keras.layers.Dense(units=1024, kernel_initializer='he_uniform'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.LeakyReLU())\n",
        "model.add(tf.keras.layers.Dropout(0.5, seed=SEED))\n",
        "model.add(tf.keras.layers.Dense(units=256, kernel_initializer='he_uniform'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.LeakyReLU())\n",
        "model.add(tf.keras.layers.Dropout(0.5, seed=SEED))\n",
        "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYXm5oxVF_-M"
      },
      "source": [
        "# Optimization params\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "lr = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "metrics = ['accuracy']\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiGKCwOTjQHa"
      },
      "source": [
        "# Visualize created model as a table\n",
        "model.summary()\n",
        "for i, layer in enumerate(inc_res.layers):\n",
        "  print(i, layer.name, \"-\", layer.trainable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WfGM1flj8Gn"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/My\\ Drive/ANN/Log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtXAkhDIkC3b"
      },
      "source": [
        "# Callbacks\n",
        "\n",
        "cwd = '/content/drive/My Drive/ANN'\n",
        "\n",
        "exps_dir = os.path.join(cwd, 'Log')\n",
        "\n",
        "if not os.path.exists(exps_dir):\n",
        "  os.makedirs(exps_dir)\n",
        "\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S') # taking instant time\n",
        "exp_name = 'FC'                                # name of experiment\n",
        "exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n",
        "\n",
        "\n",
        "if not os.path.exists(exp_dir):\n",
        "  os.makedirs(exp_dir)\n",
        "\n",
        "\n",
        "callbacks = [] \n",
        "\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), save_weights_only=True, save_best_only=True)\n",
        "\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
        "if not os.path.exists(tb_dir):\n",
        "  os.makedirs(tb_dir)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1, cooldown=0)\n",
        "callbacks.append(reduce_lr)\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=1)\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "callbacks.append(es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFw01ykWkjys"
      },
      "source": [
        "### training the model ###\n",
        "model.fit(\n",
        "    x=train_gen,\n",
        "    y=None,\n",
        "    epochs=70,\n",
        "    callbacks=callbacks,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_data=valid_gen,\n",
        "    validation_steps=len(valid_gen)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvRzPOr9k7oB"
      },
      "source": [
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'Last'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd80MGYSlE0x"
      },
      "source": [
        "# Save the model\n",
        "model.save('/content/drive/My Drive/ANN/Models/Last')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bayLst1HlMr3"
      },
      "source": [
        "#Test Time Augmentation and CSV generation\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "test_gen.reset()\n",
        "tta_steps = 10 #number of time to perform TTA\n",
        "predictions = []\n",
        "results = {}\n",
        "\n",
        "for i in tqdm(range(tta_steps)):\n",
        "    preds = model.predict(test_gen,\n",
        "                          batch_size = bs,\n",
        "                          verbose=1)\n",
        "    predictions.append(preds)\n",
        "\n",
        "pred = np.mean(predictions, axis = 0)\n",
        "predicted_class_indices = np.argmax(pred, axis=-1)\n",
        "filenames = test_gen.filenames\n",
        "prova = [e[7:] for e in filenames]\n",
        "\n",
        "for i in range(0, len(test_gen)):\n",
        "  results[prova[i]] = predicted_class_indices[i]\n",
        "\n",
        "create_csv(results=results, results_dir=test_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}